{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quarantine_covid19_behavior_analysis",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1kofYesiQd6fnKQlHtO1hARhAwbaZS5eB",
      "authorship_tag": "ABX9TyPpqlbL2BwMLYiHL9TFpnMe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcelobenedito/quarantine_covid19_behavior_analysis/blob/master/quarantine_covid19_behavior_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoKhvChd0eNA",
        "colab_type": "text"
      },
      "source": [
        "#**Quarantine Covid-19 Behavior Analysis**\n",
        "\n",
        "*It will be collect data tweets about COVID-19, quarantine and related about. This content will analysed to extract sentiment and main user behavior that makes don't stay home.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7uqUJtTtHyV",
        "colab_type": "text"
      },
      "source": [
        "**Install libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "md6SYud4tH9y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "5f12afc8-e92f-41ba-f4ac-a61735b1b881"
      },
      "source": [
        "!pip3 install unidecode\n",
        "!pip3 install googletrans\n",
        "!pip3 install twitterscraper\n",
        "!pip3 install emoji"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: googletrans in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.6/dist-packages (from googletrans) (0.13.3)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (2.10)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (0.9.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (1.1.0)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (1.4.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (2020.6.20)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (2020.7.29)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.6/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (3.2.0)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.6/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (0.9.0)\n",
            "Requirement already satisfied: contextvars>=2.1; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from sniffio->httpx==0.13.3->googletrans) (2.4)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.6/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (3.0.0)\n",
            "Requirement already satisfied: immutables>=0.9 in /usr/local/lib/python3.6/dist-packages (from contextvars>=2.1; python_version < \"3.7\"->sniffio->httpx==0.13.3->googletrans) (0.14)\n",
            "Requirement already satisfied: twitterscraper in /usr/local/lib/python3.6/dist-packages (1.6.1)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (from twitterscraper) (0.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from twitterscraper) (2.23.0)\n",
            "Requirement already satisfied: coala-utils~=0.5.0 in /usr/local/lib/python3.6/dist-packages (from twitterscraper) (0.5.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from twitterscraper) (4.2.6)\n",
            "Requirement already satisfied: billiard in /usr/local/lib/python3.6/dist-packages (from twitterscraper) (3.6.3.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4->twitterscraper) (4.6.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->twitterscraper) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->twitterscraper) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->twitterscraper) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->twitterscraper) (1.24.3)\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42176 sha256=621a9ea4613933e132f3375e3dc5f460a3936c43e7aba68acd6917eb03f17a0e\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.5.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWSeZqYvFjl3",
        "colab_type": "text"
      },
      "source": [
        "**Required imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Frum5NdVVPzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import time\n",
        "import datetime as dt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from unidecode import unidecode\n",
        "from googletrans import Translator\n",
        "from twitterscraper import query_tweets\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from emoji import demojize"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy0c6-DS9gYh",
        "colab_type": "text"
      },
      "source": [
        "**Create a funcion to search tweets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q0rre9-9hNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def search_tweets(search_filter, since, until, limit, language):\n",
        "  return query_tweets(query = search_filter, begindate = since, enddate = until, limit = limit, lang = language)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rw7hTfcGwi9",
        "colab_type": "text"
      },
      "source": [
        "**Defining filters used in search**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odq6flq8HK4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" não estou saindo \"não estou saindo\" (quarentena OR covid) (#covid-19 OR #coronavírus OR #coronavirus OR #covid OR #quarentena) lang:pt until:2020-01-31 since:2020-01-01 -filter:replies \"\"\"\n",
        "\n",
        "contains_both_words = ''\n",
        "exact_phrase = ''\n",
        "contains_any_words = '(quarentena OR covid OR coronavirus OR isolamento OR social)'\n",
        "contains_any_hashtags = ''\n",
        "no_retweet = '-filter:replies'\n",
        "language = 'pt'\n",
        "since = dt.date(2020,1,1)\n",
        "until = dt.date(2020,1,31)\n",
        "limit = 10\n",
        "\n",
        "search_filter = contains_both_words + ' ' + exact_phrase + ' ' + contains_any_words + ' ' + contains_any_hashtags + ' ' + no_retweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqNMr2yxIl6j",
        "colab_type": "text"
      },
      "source": [
        "**Extracting tweets based on search filter**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEIS2-32Ip_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets = search_tweets(search_filter, since, until, limit, language)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxeEFuANdoZQ",
        "colab_type": "text"
      },
      "source": [
        "**Transform Json to DataFrame and export to CSV file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B1HDPGido0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame({\n",
        "    'tweet_id': tweet.tweet_id, \n",
        "    'text': tweet.text,  \n",
        "    'tweet_url': tweet.tweet_url,\n",
        "    'retweets': tweet.retweets,\n",
        "    'replies': tweet.replies,\n",
        "    'is_replied': tweet.is_replied,\n",
        "    'is_reply_to': tweet.is_reply_to,\n",
        "    'user_id': tweet.user_id, \n",
        "    'created_at': tweet.timestamp\n",
        "} for tweet in tweets)\n",
        "\n",
        "df.to_csv('tweets.csv', encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf-B19NsJLJP",
        "colab_type": "text"
      },
      "source": [
        "**Printing found tweets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2dI6Q2SbYbP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "cd2329b6-710b-48c6-d1f2-103ebb913c61"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_url</th>\n",
              "      <th>retweets</th>\n",
              "      <th>replies</th>\n",
              "      <th>is_replied</th>\n",
              "      <th>is_reply_to</th>\n",
              "      <th>user_id</th>\n",
              "      <th>created_at</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1213248633954611200</td>\n",
              "      <td>Danilo Gentili e deputado do Psol discutem em ...</td>\n",
              "      <td>/aratuonline/status/1213248633954611200</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>67661509</td>\n",
              "      <td>2020-01-03 23:59:50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1213248562420703232</td>\n",
              "      <td>Jamais rebaixe alguém da família, pq essa pess...</td>\n",
              "      <td>/milamarques/status/1213248562420703232</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>256680355</td>\n",
              "      <td>2020-01-03 23:59:33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1213248430342115328</td>\n",
              "      <td>Já planejam diminuir conteúdo dos livros didát...</td>\n",
              "      <td>/rakavazquez/status/1213248430342115328</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>732761681856765952</td>\n",
              "      <td>2020-01-03 23:59:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1213248424134500353</td>\n",
              "      <td>QUESTIONÁRIO PARA 2020\\n \\nEU VOU?\\n1. Tomara ...</td>\n",
              "      <td>/gahsep1914/status/1213248424134500353</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>710262555223134208</td>\n",
              "      <td>2020-01-03 23:59:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1213248395512631296</td>\n",
              "      <td>Só os especialistas em política e economia onl...</td>\n",
              "      <td>/NerdZEEH/status/1213248395512631296</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>281665822</td>\n",
              "      <td>2020-01-03 23:58:53</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              tweet_id  ...          created_at\n",
              "0  1213248633954611200  ... 2020-01-03 23:59:50\n",
              "1  1213248562420703232  ... 2020-01-03 23:59:33\n",
              "2  1213248430342115328  ... 2020-01-03 23:59:01\n",
              "3  1213248424134500353  ... 2020-01-03 23:59:00\n",
              "4  1213248395512631296  ... 2020-01-03 23:58:53\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKPqylLAZ3PV",
        "colab_type": "text"
      },
      "source": [
        "**Open stored tweets from CSV file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78qAC2KDZ2fe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# file = open('tweets.csv', encoding='utf-8').read()\n",
        "df = pd.read_csv('tweets.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7YH5a4VcR-B",
        "colab_type": "text"
      },
      "source": [
        "**Probably will need to make a translate from Portuguese to English**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUwmT1lscSev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets = df['text']\n",
        "for tweet in tweets:\n",
        "  english_tweets = Translator().translate(unidecode(tweet)).text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ92bhg6VDuz",
        "colab_type": "text"
      },
      "source": [
        "**Data preprocessing**\n",
        "\n",
        "This process is used to preprocess the tweet text:\n",
        "\n",
        " - Tokenize words;\n",
        " - Remove all stop words; \n",
        " - Punctuaction rules; \n",
        " - Unused characters;\n",
        " - Links from tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z3s-jZIVEGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting to lowercase\n",
        "tweets = tweets.str.lower()\n",
        "\n",
        "# Removing punctuation rules\n",
        "tweets = tweets.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# TODO Removing unused links\n",
        "tweets = tweets.str.replace(r\"(http|@)\\S+\", \"\")\n",
        "\n",
        "# Remove special chars\n",
        "tweets = tweets.apply(demojize)\n",
        "tweets = tweets.str.replace(r\"::\", \": :\")\n",
        "tweets = tweets.str.replace(r\"’\", \"'\")\n",
        "tweets = tweets.str.replace(r\"[^a-z\\':_]\", \" \")\n",
        "\n",
        "# Remove repetitions\n",
        "pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n",
        "tweets = tweets.str.replace(pattern, r\"\\1\")\n",
        "\n",
        "# Transform short negation form\n",
        "tweets = tweets.str.replace(r\"(can't|cannot)\", 'can not')\n",
        "tweets = tweets.str.replace(r\"n't\", ' not')\n",
        "\n",
        "# Spliting text into words\n",
        "tokenized_words = word_tokenize(tweets, 'english')\n",
        "\n",
        "# Removing stop words\n",
        "stopwords = stopwords.words('english')\n",
        "stopwords.remove('not')\n",
        "stopwords.remove('nor')\n",
        "stopwords.remove('no')\n",
        "\n",
        "words = []\n",
        "\n",
        "for word in tokenized_word:\n",
        "  if word not in stopwords:\n",
        "    words.append(word)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}