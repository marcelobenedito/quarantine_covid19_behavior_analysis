{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quarantine_covid19_behavior_analysis",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1kofYesiQd6fnKQlHtO1hARhAwbaZS5eB",
      "authorship_tag": "ABX9TyOpEhm64Xb7+wq94VtrOpN9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcelobenedito/quarantine_covid19_behavior_analysis/blob/master/quarantine_covid19_behavior_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoKhvChd0eNA",
        "colab_type": "text"
      },
      "source": [
        "#**Quarantine Covid-19 Behavior Analysis**\n",
        "\n",
        "*It will be collect data tweets about COVID-19, quarantine and related about. This content will analysed to extract sentiment and main user behavior that makes don't stay home.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7uqUJtTtHyV",
        "colab_type": "text"
      },
      "source": [
        "**Install libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "md6SYud4tH9y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        },
        "outputId": "be9d141d-4343-4411-862b-8820dcad048d"
      },
      "source": [
        "!pip3 install unidecode\n",
        "!pip3 install googletrans\n",
        "!pip3 install twitterscraper\n",
        "!pip3 install emoji"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Collecting googletrans\n",
            "  Using cached https://files.pythonhosted.org/packages/71/3a/3b19effdd4c03958b90f40fe01c93de6d5280e03843cc5adf6956bfc9512/googletrans-3.0.0.tar.gz\n",
            "Collecting httpx==0.13.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/b4/698b284c6aed4d7c2b4fe3ba5df1fcf6093612423797e76fbb24890dd22f/httpx-0.13.3-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 1.9MB/s \n",
            "\u001b[?25hCollecting httpcore==0.9.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/d5/e4ff9318693ac6101a2095e580908b591838c6f33df8d3ee8dd953ba96a8/httpcore-0.9.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.4MB/s \n",
            "\u001b[?25hCollecting sniffio\n",
            "  Downloading https://files.pythonhosted.org/packages/b3/82/4bd4b7d9c0d1dc0fbfbc2a1e00138e7f3ab85bc239358fe9b78aa2ab586d/sniffio-1.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (3.0.4)\n",
            "Collecting rfc3986<2,>=1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/78/be/7b8b99fd74ff5684225f50dd0e865393d2265656ef3b4ba9eaaaffe622b8/rfc3986-1.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (2020.6.20)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans) (2.10)\n",
            "Collecting hstspreload\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/8b/5adb4f33ec2555122b15fdbf6c7fc5d59dafde7f0d3289adb5dee6843ad3/hstspreload-2020.7.29-py3-none-any.whl (926kB)\n",
            "\u001b[K     |████████████████████████████████| 931kB 8.4MB/s \n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.3MB/s \n",
            "\u001b[?25hCollecting h2==3.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/de/da019bcc539eeab02f6d45836f23858ac467f584bfec7a526ef200242afe/h2-3.2.0-py2.py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.3MB/s \n",
            "\u001b[?25hCollecting contextvars>=2.1; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/83/96/55b82d9f13763be9d672622e1b8106c85acb83edd7cc2fa5bc67cd9877e9/contextvars-2.4.tar.gz\n",
            "Collecting hyperframe<6,>=5.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/0c/bf88182bcb5dce3094e2f3e4fe20db28a9928cb7bd5b08024030e4b140db/hyperframe-5.2.0-py2.py3-none-any.whl\n",
            "Collecting hpack<4,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/cc/e53517f4a1e13f74776ca93271caef378dadec14d71c61c949d759d3db69/hpack-3.0.0-py2.py3-none-any.whl\n",
            "Collecting immutables>=0.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/e0/ea6fd4697120327d26773b5a84853f897a68e33d3f9376b00a8ff96e4f63/immutables-0.14-cp36-cp36m-manylinux1_x86_64.whl (98kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 9.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: googletrans, contextvars\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.0.0-cp36-none-any.whl size=15736 sha256=0a6122011f682194aaceecb62e93e52f37a48cfb72dc54b095cb91ae9aedd5ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/1a/a7/eaf4d7a3417a0c65796c547cff4deb6d79c7d14c2abd29273e\n",
            "  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for contextvars: filename=contextvars-2.4-cp36-none-any.whl size=7666 sha256=19f144b6c60ebf386484311b10cd61c14836f7a6a71a0db36027f8ce7a624ee9\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/7d/68/1ebae2668bda2228686e3c1cf16f2c2384cea6e9334ad5f6de\n",
            "Successfully built googletrans contextvars\n",
            "Installing collected packages: h11, hyperframe, hpack, h2, immutables, contextvars, sniffio, httpcore, rfc3986, hstspreload, httpx, googletrans\n",
            "Successfully installed contextvars-2.4 googletrans-3.0.0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2020.7.29 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 immutables-0.14 rfc3986-1.4.0 sniffio-1.1.0\n",
            "Requirement already satisfied: twitterscraper in /usr/local/lib/python3.6/dist-packages (1.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from twitterscraper) (2.23.0)\n",
            "Requirement already satisfied: billiard in /usr/local/lib/python3.6/dist-packages (from twitterscraper) (3.6.3.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from twitterscraper) (4.2.6)\n",
            "Requirement already satisfied: coala-utils~=0.5.0 in /usr/local/lib/python3.6/dist-packages (from twitterscraper) (0.5.1)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (from twitterscraper) (0.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->twitterscraper) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->twitterscraper) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->twitterscraper) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->twitterscraper) (2.10)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4->twitterscraper) (4.6.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.5.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWSeZqYvFjl3",
        "colab_type": "text"
      },
      "source": [
        "**Required imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Frum5NdVVPzd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5d86a01f-85f9-4f52-e37b-4b0b2be8ef7f"
      },
      "source": [
        "import string\n",
        "import time\n",
        "import datetime as dt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from unidecode import unidecode\n",
        "from googletrans import Translator\n",
        "from twitterscraper import query_tweets\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from emoji import demojize\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy0c6-DS9gYh",
        "colab_type": "text"
      },
      "source": [
        "**Create a funcion to search tweets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q0rre9-9hNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def search_tweets(search_filter, since, until, limit, language):\n",
        "  return query_tweets(query = search_filter, begindate = since, enddate = until, limit = limit, lang = language)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rw7hTfcGwi9",
        "colab_type": "text"
      },
      "source": [
        "**Defining filters used in search**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odq6flq8HK4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" não estou saindo \"não estou saindo\" (quarentena OR covid) (#covid-19 OR #coronavírus OR #coronavirus OR #covid OR #quarentena) lang:pt until:2020-01-31 since:2020-01-01 -filter:replies \"\"\"\n",
        "\n",
        "contains_both_words = ''\n",
        "exact_phrase = ''\n",
        "contains_any_words = '(quarentena OR covid OR coronavirus OR isolamento OR social)'\n",
        "contains_any_hashtags = ''\n",
        "no_retweet = '-filter:replies'\n",
        "language = 'pt'\n",
        "since = dt.date(2020,1,1)\n",
        "until = dt.date(2020,1,31)\n",
        "limit = 10\n",
        "\n",
        "search_filter = contains_both_words + ' ' + exact_phrase + ' ' + contains_any_words + ' ' + contains_any_hashtags + ' ' + no_retweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqNMr2yxIl6j",
        "colab_type": "text"
      },
      "source": [
        "**Extracting tweets based on search filter**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEIS2-32Ip_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets = search_tweets(search_filter, since, until, limit, language)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxeEFuANdoZQ",
        "colab_type": "text"
      },
      "source": [
        "**Transform Json to DataFrame and export to CSV file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B1HDPGido0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame({\n",
        "    'tweet_id': tweet.tweet_id, \n",
        "    'text': tweet.text,  \n",
        "    'tweet_url': tweet.tweet_url,\n",
        "    'retweets': tweet.retweets,\n",
        "    'replies': tweet.replies,\n",
        "    'is_replied': tweet.is_replied,\n",
        "    'is_reply_to': tweet.is_reply_to,\n",
        "    'user_id': tweet.user_id, \n",
        "    'created_at': tweet.timestamp\n",
        "} for tweet in tweets)\n",
        "\n",
        "df.to_csv('tweets.csv', encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf-B19NsJLJP",
        "colab_type": "text"
      },
      "source": [
        "**Printing found tweets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2dI6Q2SbYbP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "cd2329b6-710b-48c6-d1f2-103ebb913c61"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_url</th>\n",
              "      <th>retweets</th>\n",
              "      <th>replies</th>\n",
              "      <th>is_replied</th>\n",
              "      <th>is_reply_to</th>\n",
              "      <th>user_id</th>\n",
              "      <th>created_at</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1213248633954611200</td>\n",
              "      <td>Danilo Gentili e deputado do Psol discutem em ...</td>\n",
              "      <td>/aratuonline/status/1213248633954611200</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>67661509</td>\n",
              "      <td>2020-01-03 23:59:50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1213248562420703232</td>\n",
              "      <td>Jamais rebaixe alguém da família, pq essa pess...</td>\n",
              "      <td>/milamarques/status/1213248562420703232</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>256680355</td>\n",
              "      <td>2020-01-03 23:59:33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1213248430342115328</td>\n",
              "      <td>Já planejam diminuir conteúdo dos livros didát...</td>\n",
              "      <td>/rakavazquez/status/1213248430342115328</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>732761681856765952</td>\n",
              "      <td>2020-01-03 23:59:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1213248424134500353</td>\n",
              "      <td>QUESTIONÁRIO PARA 2020\\n \\nEU VOU?\\n1. Tomara ...</td>\n",
              "      <td>/gahsep1914/status/1213248424134500353</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>710262555223134208</td>\n",
              "      <td>2020-01-03 23:59:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1213248395512631296</td>\n",
              "      <td>Só os especialistas em política e economia onl...</td>\n",
              "      <td>/NerdZEEH/status/1213248395512631296</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>281665822</td>\n",
              "      <td>2020-01-03 23:58:53</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              tweet_id  ...          created_at\n",
              "0  1213248633954611200  ... 2020-01-03 23:59:50\n",
              "1  1213248562420703232  ... 2020-01-03 23:59:33\n",
              "2  1213248430342115328  ... 2020-01-03 23:59:01\n",
              "3  1213248424134500353  ... 2020-01-03 23:59:00\n",
              "4  1213248395512631296  ... 2020-01-03 23:58:53\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKPqylLAZ3PV",
        "colab_type": "text"
      },
      "source": [
        "**Open stored tweets from CSV file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78qAC2KDZ2fe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# file = open('tweets.csv', encoding='utf-8').read()\n",
        "df = pd.read_csv('tweets.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7YH5a4VcR-B",
        "colab_type": "text"
      },
      "source": [
        "**Probably will need to make a translate from Portuguese to English**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUwmT1lscSev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets = df['text']\n",
        "for tweet in tweets:\n",
        "  english_tweets = Translator().translate(unidecode(tweet)).text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ92bhg6VDuz",
        "colab_type": "text"
      },
      "source": [
        "**Data preprocessing**\n",
        "\n",
        "This process is used to preprocess the tweet text:\n",
        "\n",
        " - Tokenize words;\n",
        " - Remove all stop words; \n",
        " - Punctuaction rules; \n",
        " - Unused characters;\n",
        " - Links from tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK1rn3aPP6Sw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "af74584e-cc9f-4681-c59b-84ad0f71eff3"
      },
      "source": [
        "tweets = ['The second-quarter contraction set a grim record, and it would have been worse without government aid that is expiring.', \n",
        "          'The coronavirus pandemic’s toll on the nation’s economy became emphatically clearer Thursday as the government detailed the most devastating three-month collapse on record, which wiped away nearly five years of growth.']\n",
        "tweets = pd.DataFrame(tweets, columns=['text'])\n",
        "tweets = tweets.text\n",
        "tweets"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    The second-quarter contraction set a grim reco...\n",
              "1    The coronavirus pandemic’s toll on the nation’...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z3s-jZIVEGS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c8f9b46a-2b3b-402c-ef9e-e4337ffd616d"
      },
      "source": [
        "# Converting to lowercase\n",
        "tweets = tweets.str.lower()\n",
        "\n",
        "# Removing punctuation rules\n",
        "tweets = tweets.str.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# Removing unused links\n",
        "tweets = tweets.str.replace(r\"(http|@)\\S+\", \"\")\n",
        "\n",
        "# Remove special chars\n",
        "tweets = tweets.apply(demojize)\n",
        "tweets = tweets.str.replace(r\"::\", \": :\")\n",
        "tweets = tweets.str.replace(r\"’\", \"'\")\n",
        "tweets = tweets.str.replace(r\"[^a-z\\':_]\", \" \")\n",
        "\n",
        "# Remove repetitions\n",
        "pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n",
        "tweets = tweets.str.replace(pattern, r\"\\1\")\n",
        "\n",
        "# Transform short negation form\n",
        "tweets = tweets.str.replace(r\"(can't|cannot)\", 'can not')\n",
        "tweets = tweets.str.replace(r\"n't\", ' not')\n",
        "\n",
        "# Spliting text into words\n",
        "# tweets = word_tokenize(tweets, 'english')\n",
        "\n",
        "# Removing stop words\n",
        "stopwords = stopwords.words('english')\n",
        "stopwords.remove('not')\n",
        "stopwords.remove('nor')\n",
        "stopwords.remove('no')\n",
        "\n",
        "tweets.apply(\n",
        "    lambda tweet: ' '.join([word for word in tweet.split() if word not in stopwords])\n",
        ")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    secondquarter contraction set grim record woul...\n",
              "1    coronavirus pandemics toll nations economy bec...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    }
  ]
}